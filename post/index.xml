<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Squirrels</title>
    <link>http://eliquious.github.io/post/</link>
    <description>Recent content in Posts on Squirrels</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Jan 2016 20:18:01 -0600</lastBuildDate>
    <atom:link href="http://eliquious.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Writing a database in Go: Part 3</title>
      <link>http://eliquious.github.io/post/writing-a-database-part-3/</link>
      <pubDate>Sun, 31 Jan 2016 20:18:01 -0600</pubDate>
      
      <guid>http://eliquious.github.io/post/writing-a-database-part-3/</guid>
      <description>

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;This is a continuation of the &lt;a href=&#34;https://eliquious.github.io/post/writing-a-database-part-2/&#34; title=&#34;Part 2: Lexing a query language&#34;&gt;second part&lt;/a&gt; of this series where we discussed lexing the query language for this new database. In this third part we will write our parser.&lt;/p&gt;

&lt;h3 id=&#34;parsing:adc9367f9fa0516fa308d9b9c785dca8&#34;&gt;Parsing&lt;/h3&gt;

&lt;p&gt;We start by defining the &lt;code&gt;Parser&lt;/code&gt;. A parser takes the tokens produced by the lexer and creates AST nodes which can then be used by a query executor.&lt;/p&gt;

&lt;p&gt;For PrefixDB, our parser is  a simple struct with a &lt;code&gt;TokenBuffer&lt;/code&gt;. We can also define a few helper methods to start with as well. We can create a new &lt;code&gt;Parser&lt;/code&gt; from an &lt;code&gt;io.Reader&lt;/code&gt; or from a string. After the parser has been created we use the &lt;code&gt;ParseStatement&lt;/code&gt; function to initiate the parsing. It also reads the first token and delegates which specific parsing function to use for this query.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;parseString&lt;/code&gt; and &lt;code&gt;parseIndent&lt;/code&gt; functions validate the next token as either a string or an identifier. The &lt;code&gt;scan&lt;/code&gt;, &lt;code&gt;unscan&lt;/code&gt;, &lt;code&gt;peekRune&lt;/code&gt; and &lt;code&gt;scanIgnoreWhitespace&lt;/code&gt; methods help manage the token buffer such as advancing the parser to the next token, etc..&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package parser

import (
    &amp;quot;io&amp;quot;
    &amp;quot;strings&amp;quot;

    &amp;quot;github.com/eliquious/lexer&amp;quot;
    tokens &amp;quot;github.com/eliquious/prefixdb/lexer&amp;quot;
)

// Parser represents an PrefixDB parser.
type Parser struct {
    s *lexer.TokenBuffer
}

// NewParser returns a new instance of Parser.
func NewParser(r io.Reader) *Parser {
    return &amp;amp;Parser{s: lexer.NewTokenBuffer(r)}
}

// ParseString parses a statement string and returns its AST representation.
func ParseString(s string) (Node, error) {
    return NewParser(strings.NewReader(s)).ParseStatement()
}

// ParseStatement parses a string and returns a Node AST object.
func (p *Parser) ParseStatement() (Node, error) {

    // Inspect the first token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    switch tok {
    case tokens.CREATE:
        return p.parseCreateStatement()
    case tokens.DROP:
        return p.parseDropStatement()
    case tokens.SELECT:
        return p.parseSelectStatement()
    case tokens.DELETE:
        return p.parseDeleteStatement()
    case tokens.UPSERT:
        return p.parseUpsertStatement()
    default:
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;CREATE&amp;quot;, &amp;quot;DROP&amp;quot;, &amp;quot;SELECT&amp;quot;, &amp;quot;DELETE&amp;quot;, &amp;quot;UPSERT&amp;quot;}, pos)
    }
}

// parserString parses a string.
func (p *Parser) parseString() (string, error) {
    tok, pos, lit := p.scanIgnoreWhitespace()
    if tok != lexer.STRING {
        return &amp;quot;&amp;quot;, NewParseError(tokstr(tok, lit), []string{&amp;quot;string&amp;quot;}, pos)
    }
    return lit, nil
}

// parseIdent parses an identifier.
func (p *Parser) parseIdent() (string, error) {
    tok, pos, lit := p.scanIgnoreWhitespace()
    if tok != lexer.IDENT {
        p.unscan()
        return &amp;quot;&amp;quot;, NewParseError(tokstr(tok, lit), []string{&amp;quot;identifier&amp;quot;}, pos)
    }
    return lit, nil
}

// scan returns the next token from the underlying scanner.
func (p *Parser) scan() (tok lexer.Token, pos lexer.Pos, lit string) { return p.s.Scan() }

// unscan pushes the previously read token back onto the buffer.
func (p *Parser) unscan() { p.s.Unscan() }

// peekRune returns the next rune that would be read by the scanner.
func (p *Parser) peekRune() rune { return p.s.Peek() }

// scanIgnoreWhitespace scans the next non-whitespace token.
func (p *Parser) scanIgnoreWhitespace() (tok lexer.Token, pos lexer.Pos, lit string) {
    tok, pos, lit = p.scan()
    if tok == lexer.WS {
        tok, pos, lit = p.scan()
    }
    return
}

// tokstr returns a string based on the token provided
func tokstr(tok lexer.Token, lit string) string {
    if tok == lexer.IDENT {
        return &amp;quot;IDENTIFIER (&amp;quot; + lit + &amp;quot;)&amp;quot;
    }
    return tok.String()
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all we need to start writing the specific query types. Now that we have a parser we can now define the AST node interface. The &lt;code&gt;Node&lt;/code&gt; interface is common for all queries.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Node interface {
    NodeType() NodeType
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The interface provides a way to get a node&amp;rsquo;s type as well as a method for printing the query. The &lt;code&gt;NodeType&lt;/code&gt; is a simply enum and defined like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type NodeType int

const (
    CreateKeyspaceType NodeType = iota
    DropKeyspaceType
    SelectType
    UpsertType
    DeleteType
    StringLiteralType
    StringLiteralGroupType
    ExpressionType
    KeyAttributeType
    BetweenType
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The AST ndoes are simple structs which fulfill the &lt;code&gt;Node&lt;/code&gt; interface. Let&amp;rsquo;s start with the &lt;code&gt;CREATE&lt;/code&gt; query.&lt;/p&gt;

&lt;h4 id=&#34;create-keyspace:adc9367f9fa0516fa308d9b9c785dca8&#34;&gt;CREATE KEYSPACE&lt;/h4&gt;

&lt;p&gt;If you remember from the first part of this series, a &lt;code&gt;CREATE KEYSPACE&lt;/code&gt; query might look something like this: &lt;code&gt;CREATE KEYSPACE users WITH KEYS username&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In order to represent that query in a struct we need to store the keyspace name as well as the keys. It might look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CreateStatement struct {
    Keyspace string
    Keys     []string
}

func (CreateStatement) NodeType() NodeType {
    return CreateKeyspaceType
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The struct also needs the &lt;code&gt;String()&lt;/code&gt; method to fully implement the Node interface, but it simply recreates the query string from the struct.&lt;/p&gt;

&lt;p&gt;Now that we have an AST node we can parse the query. In order to fully parse the &lt;code&gt;CREATE&lt;/code&gt; statement we need to define four new methods. First we need to define the top level method for the query. It inspecs the first token after the &lt;code&gt;CREATE&lt;/code&gt; keyword, which should be &lt;code&gt;KEYSPACE&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If &lt;code&gt;KEYSPACE&lt;/code&gt; is found it will continue parsing the query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseCreateStatement parses a string and returns an AST object.
// This function assumes the &amp;quot;CREATE&amp;quot; token has already been consumed.
func (p *Parser) parseCreateStatement() (Node, error) {

    // Inspect the first token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    switch tok {
    case tokens.KEYSPACE:
        return p.parseCreateKeyspaceStatement()
    default:
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;KEYSPACE&amp;quot;}, pos)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the keywords have been consumed, we need the read the name of the keyspace and keys. Once the keyspace name is determined, we verify the &lt;code&gt;WITH KEY(S)&lt;/code&gt; keywords, then the name of the keys.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseCreateKeyspaceStatement parses a string and returns a CreateKeyspaceStatement.
// This function assumes the &amp;quot;CREATE&amp;quot; token has already been consumed.
func (p *Parser) parseCreateKeyspaceStatement() (*CreateStatement, error) {
    stmt := &amp;amp;CreateStatement{}

    // Parse the name of the keyspace to be used
    lit, err := p.parseKeyspace()
    if err != nil {
        return nil, err
    }
    stmt.Keyspace = lit

    // Inspect the WITH token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    switch tok {
    case tokens.WITH:

        // Inspect the KEY or KEYS token.
        tok, pos, lit := p.scanIgnoreWhitespace()
        switch tok {
        case tokens.KEY:
            k, err := p.parseIdent()
            if err != nil {
                return nil, err
            } else {
                stmt.Keys = append(stmt.Keys, k)
            }
        case tokens.KEYS:
            k, err := p.parseIdentList()
            if err != nil {
                return nil, err
            } else {
                stmt.Keys = k
            }
        default:
            return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;KEY&amp;quot;, &amp;quot;KEYS&amp;quot;}, pos)
        }
    default:
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;WITH&amp;quot;}, pos)
    }

    // Verify end of query
    tok, pos, lit = p.scanIgnoreWhitespace()
    switch tok {
    case lexer.EOF:
    case lexer.SEMICOLON:
    default:
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;EOF&amp;quot;, &amp;quot;SEMICOLON&amp;quot;}, pos)
    }

    return stmt, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To parse the name of the keyspace we need a new method:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseKeyspace returns a keyspace title or an error
func (p *Parser) parseKeyspace() (string, error) {
    var keyspace string
    tok, pos, lit := p.scanIgnoreWhitespace()
    if tok != lexer.IDENT {
        return &amp;quot;&amp;quot;, NewParseError(tokstr(tok, lit), []string{&amp;quot;keyspace&amp;quot;}, pos)
    }
    keyspace = lit

    // Scan entire keyspace
    // Keyspaces are a period delimited list of identifiers
    var endPeriod bool
    for {
        tok, pos, lit = p.scan()
        if tok == lexer.DOT {
            keyspace += &amp;quot;.&amp;quot;
            endPeriod = true
        } else if tok == lexer.IDENT {
            keyspace += lit
            endPeriod = false
        } else {
            break
        }
    }

    // remove last token
    p.unscan()

    // Keyspaces can&#39;t end on a period
    if endPeriod {
        return &amp;quot;&amp;quot;, NewParseError(tokstr(tok, lit), []string{&amp;quot;identifier&amp;quot;}, pos)
    }
    return keyspace, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Keyspace names can be a single identifier, or several identifiers divided by a period. Identifiers can also have underscores but not spaces.&lt;/p&gt;

&lt;p&gt;To wrap up the &lt;code&gt;CREATE&lt;/code&gt; statement we need a method to parse a list of key names for the keyspace. Keys are identifiers that are optionally separated by a comma.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseIdentList returns a list of attributes or an error
func (p *Parser) parseIdentList() ([]string, error) {
    var keys []string
    tok, pos, lit := p.scanIgnoreWhitespace()
    if tok != lexer.IDENT {
        return keys, NewParseError(tokstr(tok, lit), []string{&amp;quot;identifier&amp;quot;}, pos)
    }
    keys = append(keys, lit)

    // Scan entire list
    // Key lists are comma delimited
    for {
        tok, pos, lit = p.scanIgnoreWhitespace()
        if tok == lexer.EOF {
            break
        } else if tok != lexer.COMMA {
            return keys, NewParseError(tokstr(tok, lit), []string{&amp;quot;COMMA&amp;quot;, &amp;quot;EOF&amp;quot;}, pos)
        }

        k, err := p.parseIdent()
        if err != nil {t
            return keys, err
        }
        keys = append(keys, k)
    }

    // remove last token
    p.unscan()

    return keys, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all we need to parse the &lt;code&gt;CREATE&lt;/code&gt; statement. The &lt;code&gt;DROP&lt;/code&gt; statement is very similar to &lt;code&gt;CREATE&lt;/code&gt; except for the list of keys so we&amp;rsquo;re going to skip it. You can always find it in the repo if you want to see it.&lt;/p&gt;

&lt;h4 id=&#34;select:adc9367f9fa0516fa308d9b9c785dca8&#34;&gt;SELECT&lt;/h4&gt;

&lt;p&gt;The &lt;code&gt;SELECT&lt;/code&gt; statement is perhaps the most difficult to parse so we&amp;rsquo;ll do it next. The select query looks fairly simple but parsing the &lt;code&gt;WHERE&lt;/code&gt; clause might cause you greif. A sample query might look like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;SELECT FROM users.settings WHERE user = &amp;quot;eliquious&amp;quot; AND setting = &amp;quot;email&amp;quot;;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;There are 2 main parts to the select query: the keyspace and the where clause. So our AST node looks like this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SelectStatement struct {
    Keyspace string
    Where    []Expression
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can start parsing the query by defining the top level function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseSelectStatement parses a string and returns an AST object.
// This function assumes the &amp;quot;SELECT&amp;quot; token has already been consumed.
func (p *Parser) parseSelectStatement() (Node, error) {

    ks, where, err := p.parseFromWhere()
    if err != nil {
        return nil, err
    }

    return &amp;amp;SelectStatement{
        Keyspace: ks,
        Where:    where,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the &lt;code&gt;SELECT&lt;/code&gt; keyword we have &lt;code&gt;FROM&lt;/code&gt; and then the keyspace name. After the keyspace name, we can parse the &lt;code&gt;WHERE&lt;/code&gt; clause.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Parser) parseFromWhere() (string, []Expression, error) {
    var keyspace string
    var exprs []Expression

    // Inspect the FROM token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    switch tok {
    case tokens.FROM:

        // Parse the name of the keyspace to be used
        lit, err := p.parseKeyspace()
        if err != nil {
            return &amp;quot;&amp;quot;, nil, err
        }
        keyspace = lit

        // Inspect the WHERE token.
        tok, pos, lit := p.scanIgnoreWhitespace()
        switch tok {
        case tokens.WHERE:

            // Parse the WHERE clause
            exp, err := p.parseWhereClause(true, true)
            if err != nil {
                return &amp;quot;&amp;quot;, nil, err
            }
            exprs = exp

        default:
            return &amp;quot;&amp;quot;, nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;WHERE&amp;quot;}, pos)
        }

    default:
        return &amp;quot;&amp;quot;, nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;FROM&amp;quot;}, pos)
    }
    return keyspace, exprs, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parsing the &lt;code&gt;WHERE&lt;/code&gt; is interesting and because the &lt;code&gt;UPSERT&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt; queries all have similar clauses, we can reuse this method. The &lt;code&gt;UPSERT&lt;/code&gt; query does not make use of &lt;code&gt;BETWEEN&lt;/code&gt; or a logical &lt;code&gt;OR&lt;/code&gt; so we can disable them by arguments.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseWhereClause parses a string and returns an AST object.
// This function assumes the &amp;quot;WHERE&amp;quot; token has already been consumed.
func (p *Parser) parseWhereClause(allowBetween bool, allowLogicalOR bool) ([]Expression, error) {
    var expr []Expression

    // Read expression
    exp, err := p.parseExpression(allowBetween, allowLogicalOR)
    if err != nil {
        return expr, err
    }
    expr = append(expr, exp)

OUTER:
    for {

        // Test if there is another expression
        tok, pos, lit := p.scanIgnoreWhitespace()
        switch tok {
        case lexer.EOF, lexer.SEMICOLON:
            break OUTER
        case lexer.AND:

            exp, err := p.parseExpression(allowBetween, allowLogicalOR)
            if err != nil {
                return expr, err
            }
            expr = append(expr, exp)

        default:
            return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;EOF&amp;quot;, &amp;quot;SEMICOLON&amp;quot;, &amp;quot;AND&amp;quot;}, pos)
        }
    }
    return expr, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The allowed conditions are not as full featured as a traditional relational database due to the decisions made when creating the data model. Each expression is &lt;code&gt;AND&lt;/code&gt;d with the next. The only place an &lt;code&gt;OR&lt;/code&gt; is allowed is inside the equality operator (&lt;code&gt;attr = &amp;quot;name2&amp;quot; OR &amp;quot;name2&amp;quot;&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;There are only 2 real types of expression operators: the equality and between operators. We can decide which expression is next via the following method. First we parse the key name, then the operator and values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// parseExpression parses a string and returns an AST object.
func (p *Parser) parseExpression(allowBetween, allowLogicalOR bool) (Expression, error) {

    // Inspect the FROM token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    if tok != lexer.IDENT {
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;identifier&amp;quot;}, pos)
    }
    ident := lit

    // Inspect the operator token.
    tok, pos, lit = p.scanIgnoreWhitespace()
    switch tok {
    case lexer.EQ:
        expr, err := p.parseEqualityExpression(ident, allowLogicalOR)
        if err != nil {
            return nil, err
        }
        return expr, nil
    case tokens.BETWEEN:
        if !allowBetween {
            return nil, &amp;amp;ParseError{Message: &amp;quot;BETWEEN not allowed&amp;quot;, Pos: pos}
        }

        expr, err := p.parseBetweenExpression(ident)
        if err != nil {
            return nil, err
        }
        return expr, nil
    default:
        if allowBetween {
            return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;EQ&amp;quot;, &amp;quot;BETWEEN&amp;quot;}, pos)
        }
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;EQ&amp;quot;}, pos)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To parse the equality expression we define the method below. As mentioned above the equality operator can have 2 values. If after the first value, there can be an &lt;code&gt;AND&lt;/code&gt; or an &lt;code&gt;OR&lt;/code&gt; keyword.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// EqualityExpression represents a filter condition which filters based on an exact match.
type EqualityExpression struct {
    KeyAttribute string
    Value        Node
}

...

// parseEqualityExpression parses a string and returns an AST object.
func (p *Parser) parseEqualityExpression(ident string, allowLogicalOR bool) (Expression, error) {
    // expr := &amp;amp;EqualityExpression{KeyAttribute: ident}
    // var expr Expression

    // Parse the string value
    value, err := p.parseString()
    if err != nil {
        return nil, err
    }

    // Inspect the AND / OR token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    switch tok {
    case lexer.AND, lexer.EOF:
        p.unscan()
        return EqualityExpression{KeyAttribute: ident, Value: StringLiteral{value}}, nil
    case lexer.OR:
        // Upserts cannot contain OR equality clauses.
        if !allowLogicalOR {
            return nil, &amp;amp;ParseError{Message: &amp;quot;OR not allowed&amp;quot;, Pos: pos}
        }

        values := []string{value}

        // Parse the string value
        value, err := p.parseString()
        if err != nil {
            return nil, err
        }
        values = append(values, value)
        return EqualityExpression{KeyAttribute: ident, Value: StringLiteralGroup{Operator: OrOperator, Values: values}}, nil
    default:
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;AND&amp;quot;, &amp;quot;OR&amp;quot;}, pos)
    }
}

// BetweenExpression represents a filter condition which filters keys between 2 given strings.
type BetweenExpression struct {
    KeyAttribute string
    Values       StringLiteralGroup
}

...

// parseBetweenExpression parses a string and returns an AST object.
func (p *Parser) parseBetweenExpression(ident string) (Expression, error) {
    expr := BetweenExpression{KeyAttribute: ident}

    // Parse the string value
    value, err := p.parseString()
    if err != nil {
        return nil, err
    }

    // Inspect the AND / OR token.
    tok, pos, lit := p.scanIgnoreWhitespace()
    switch tok {
    case lexer.AND:
        values := []string{value}

        // Parse the string value
        value, err := p.parseString()
        if err != nil {
            return nil, err
        }
        values = append(values, value)
        expr.Values = StringLiteralGroup{Operator: AndOperator, Values: values}
    default:
        return nil, NewParseError(tokstr(tok, lit), []string{&amp;quot;AND&amp;quot;}, pos)
    }
    return expr, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;errors:adc9367f9fa0516fa308d9b9c785dca8&#34;&gt;Errors&lt;/h4&gt;

&lt;p&gt;One thing I have left out, besides the other queries, is how errors are handled. I&amp;rsquo;ve defined a single error type for all parse errors which is called ironically a &lt;code&gt;ParseError&lt;/code&gt;. It is fairly straight forward as it will return a string whieh shows the current token vs. what was expected.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// ParseError represents an error that occurred during parsing.
type ParseError struct {
    Message  string
    Found    string
    Expected []string
    Pos      lexer.Pos
}

// newParseError returns a new instance of ParseError.
func NewParseError(found string, expected []string, pos lexer.Pos) *ParseError {
    return &amp;amp;ParseError{Found: found, Expected: expected, Pos: pos}
}

// Error returns the string representation of the error.
func (e *ParseError) Error() string {
    if e.Message != &amp;quot;&amp;quot; {
        return fmt.Sprintf(&amp;quot;%s at line %d, char %d&amp;quot;, e.Message, e.Pos.Line+1, e.Pos.Char+1)
    }
    return fmt.Sprintf(&amp;quot;found %s, expected %s at line %d, char %d&amp;quot;, e.Found, strings.Join(e.Expected, &amp;quot;, &amp;quot;), e.Pos.Line+1, e.Pos.Char+1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;conclusion:adc9367f9fa0516fa308d9b9c785dca8&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This turned out the be a very long post despite not including all of the query types. However, the additional queries use the same techniques and helper methods as the &lt;code&gt;CREATE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt; statements. If you want to checkout the rest of the parser as well as how to test one, you can find the code in the &lt;a href=&#34;http://github.com/eliquious/prefixdb/&#34; title=&#34;PrefixDB on GitHub&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a database in Go: Part 2</title>
      <link>http://eliquious.github.io/post/writing-a-database-part-2/</link>
      <pubDate>Sat, 16 Jan 2016 20:02:44 -0600</pubDate>
      
      <guid>http://eliquious.github.io/post/writing-a-database-part-2/</guid>
      <description>

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;p&gt;This is a continuation of the &lt;a href=&#34;https://eliquious.github.io/post/writing-a-database-part-1/&#34; title=&#34;Part 1: Data model and query language&#34;&gt;first part&lt;/a&gt; of this series where we discussed the data model and query language for this new database. In this second part we will setup our lexer.&lt;/p&gt;

&lt;h3 id=&#34;lexing:753b520aaeb3b1a1101bfc74aa667f0c&#34;&gt;Lexing&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Lexical_analysis/&#34; title=&#34;Wikipedia: Lexical Analysis&#34;&gt;Lexing&lt;/a&gt; is the process for transforming a textual input into tokens representing each keyword, character or puncuation. Fortunately, there&amp;rsquo;s little we have to do for lexing the input due to a &lt;a href=&#34;https://github.com/eliquious/lexer&#34;&gt;library&lt;/a&gt; I have written previous.&lt;/p&gt;

&lt;p&gt;The library already handles a large majority of the syntax of the query language we designed previously. However, we will need to create the keyword tokens. We revist the first post and simply record each keyword as a constant.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package lexer

import (
    &amp;quot;github.com/eliquious/lexer&amp;quot;
)

const (
    // Starts the keywords with an offset from the built in tokens so our new tokens don&#39;t overlap.
    startKeywords lexer.Token = iota + 1000

    // CREATE starts a CREATE KEYSPACE query.
    CREATE

    // SELECT starts a SELECT FROM query.
    SELECT

    // DELETE deletes keys from a keyspace.
    DELETE

    // DROP deletes an entire keyspace.
    DROP

    // FROM specifies which keyspace to select and delete from.
    FROM

    // WHERE allows for key filtering when selecting or deleting.
    WHERE

    // KEYSPACE signifies what is being created.
    KEYSPACE

    // WITH is used to prefix query options.
    WITH

    // KEY signifies a key attribute follows.
    KEY

    // KEYS signifies several key attributes follow.
    KEYS
    endKeywords

    // Separates the keywords from the conditionals
    startConditionals

    // BETWEEN filters an attribute by two values.
    BETWEEN
    endConditionals
)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that the tokens are created we need to load them into the lexer. This is super easy to do. We just create a map of the tokens we want to add and call &lt;code&gt;lexer.LoadTokenMap()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    // Loads keyword tokens into lexer
    lexer.LoadTokenMap(tokenMap)
}

var tokenMap = map[lexer.Token]string{
    CREATE:   &amp;quot;CREATE&amp;quot;,
    SELECT:   &amp;quot;SELECT&amp;quot;,
    DELETE:   &amp;quot;DELETE&amp;quot;,
    DROP:     &amp;quot;DROP&amp;quot;,
    FROM:     &amp;quot;FROM&amp;quot;,
    WHERE:    &amp;quot;WHERE&amp;quot;,
    KEYSPACE: &amp;quot;KEYSPACE&amp;quot;,
    WITH:     &amp;quot;WITH&amp;quot;,
    KEY:      &amp;quot;KEY&amp;quot;,
    BETWEEN:  &amp;quot;BETWEEN&amp;quot;,
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;testing-the-lexer:753b520aaeb3b1a1101bfc74aa667f0c&#34;&gt;Testing the lexer&lt;/h4&gt;

&lt;p&gt;Now we need to test to see if the keywords have been loaded and the lexer can lex our input. To test the lexer we can write a short program to read from &lt;code&gt;os.Stdin&lt;/code&gt; and print the tokens as well as their locations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
        &amp;quot;flag&amp;quot;
        &amp;quot;fmt&amp;quot;
        &amp;quot;os&amp;quot;
        &amp;quot;strconv&amp;quot;

        &amp;quot;github.com/eliquious/lexer&amp;quot;
        _ &amp;quot;github.com/eliquious/prefixdb/lexer&amp;quot;
)

var skipWhitespace = flag.Bool(&amp;quot;w&amp;quot;, false, &amp;quot;skip whitespace tokens&amp;quot;)

func main() {
        flag.Parse()
        l := lexer.NewScanner(os.Stdin)
        for {
                tok, pos, lit := l.Scan()

                // exit if EOF
                if tok == lexer.EOF {
                        break
                }

                // skip whitespace tokens
                if tok == lexer.WS &amp;amp;&amp;amp; *skipWhitespace {
                        continue
                }

                // Print token
                if len(lit) &amp;gt; 0 {
                        fmt.Printf(&amp;quot;[%4d:%-3d] %10s - %s\n&amp;quot;, pos.Line, pos.Char, tok, strconv.QuoteToASCII(lit))
                } else {
                        fmt.Printf(&amp;quot;[%4d:%-3d] %10s\n&amp;quot;, pos.Line, pos.Char, tok)
                }
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have a simple tool, let&amp;rsquo;s test some queries.&lt;/p&gt;

&lt;h4 id=&#34;lexer-output:753b520aaeb3b1a1101bfc74aa667f0c&#34;&gt;Lexer Output&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;CREATE KEYSPACE&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;CREATE KEYSPACE users WITH KEY username&amp;quot; | ./lexer -w=true
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[   0:0  ]     CREATE
[   0:7  ]   KEYSPACE
[   0:16 ]      IDENT - &amp;quot;users&amp;quot;
[   0:22 ]       WITH
[   0:27 ]        KEY
[   0:31 ]      IDENT - &amp;quot;username&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;SELECT&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;SELECT FROM users WHERE username = &#39;eliquious&#39;&amp;quot; | ./lexer -w=true
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[   0:0  ]     SELECT
[   0:7  ]       FROM
[   0:12 ]      IDENT - &amp;quot;users&amp;quot;
[   0:18 ]      WHERE
[   0:24 ]      IDENT - &amp;quot;username&amp;quot;
[   0:33 ]          =
[   0:34 ]    TEXTUAL - &amp;quot;eliquious&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I could show all the queries, but I&amp;rsquo;ll let you run the rest of them. Setting up the lexer didn&amp;rsquo;t take much time at all, but the parser and AST nodes will take a while longer. We&amp;rsquo;ll tackle those next time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a database in Go: Part 1</title>
      <link>http://eliquious.github.io/post/writing-a-database-part-1/</link>
      <pubDate>Fri, 15 Jan 2016 00:21:42 -0600</pubDate>
      
      <guid>http://eliquious.github.io/post/writing-a-database-part-1/</guid>
      <description>

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Databases are a dime a dozen these days. So why in the world would I write another one? Well, mainly for fun, but also for specialization. Building a database which specializes in one thing could reap huge benefits such as performance, data model, or even operations.&lt;/p&gt;

&lt;p&gt;So what kind of database are we building? We&amp;rsquo;re going to build yet another key-value database. However, this one is going to be geared specifically for range scans and is called &lt;a href=&#34;http://github.com/eliquious/prefixdb/&#34; title=&#34;PrefixDB on GitHub&#34;&gt;PrefixDB&lt;/a&gt;. So where should we start? We should start by defining the data model and then the query language.&lt;/p&gt;

&lt;h3 id=&#34;defining-the-data-model:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;Defining the data model&lt;/h3&gt;

&lt;p&gt;If we want to do range scans, where do we start? Or maybe a better question is how do we want to query the data? Here are some questions that still need answering:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What about data types? Are we going to have them?&lt;/li&gt;
&lt;li&gt;Is there a schema for the keys or values?&lt;/li&gt;
&lt;li&gt;What about multidimensional range scans?&lt;/li&gt;
&lt;li&gt;Does the database need to filter the keys or values by attributes?&lt;/li&gt;
&lt;li&gt;How are the keys and values going to be stored?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all of those questions will need to be answered, let&amp;rsquo;s start with the basics. For PrefixDB, both the keys and values will be strings for simplicity. What about schemas? Well, that might depend on if we want to include multidimensional keys. Allowing for multidimensional keys would allow for more flexibility but also help distinguish PrefixDB from all the other options. I&amp;rsquo;d say let&amp;rsquo;s go for it.&lt;/p&gt;

&lt;p&gt;So if we&amp;rsquo;re going to do multidimensional keys, what does that mean for a schema? For values, it doesn&amp;rsquo;t really make sense, but at a minimum we probably need to validate the dimensions used in the keys.&lt;/p&gt;

&lt;p&gt;What about data storage? For range scans we need something that can do efficient reads and writes. Fortunately, there&amp;rsquo;s the excellent &lt;a href=&#34;http://github.com/boltdb/bolt/&#34; title=&#34;BoltDB on GitHub&#34;&gt;BoltDB&lt;/a&gt; we might can take advantage of as our initial storage engine.&lt;/p&gt;

&lt;p&gt;BoltDB is a simple (but awesome!) B+Tree implementation in pure Go. I&amp;rsquo;ve used it in several projects and found it to be quite a joy to use. Not to mention the performance is incredible for range scans.&lt;/p&gt;

&lt;h3 id=&#34;defining-the-query-language:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;Defining the query language&lt;/h3&gt;

&lt;p&gt;Now that we have a somewhat defined data model, let&amp;rsquo;s start looking at the query language. From my experience, defining the query language (both the syntax and semantics) can help ferret out specific details for the data model.&lt;/p&gt;

&lt;p&gt;So if we&amp;rsquo;re going to use multidimensional keys, then we need a way to query the keys by multiple attributes as well as define what those attributes are going to be. Let&amp;rsquo;s start by defining a keyspace.&lt;/p&gt;

&lt;h4 id=&#34;create-keyspace:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;CREATE KEYSPACE&lt;/h4&gt;

&lt;p&gt;Here&amp;rsquo;s the syntax for creating keyspaces. A keyspace, at least for the purposes of PrefixDB, is simply a collection of key-value pairs which belong to a set of attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// This will create a `users` keyspace with a single key.
CREATE KEYSPACE users WITH KEY username
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, what about multiple attributes for keys? Well, there&amp;rsquo;s a few ways we can do it but I think I&amp;rsquo;ve settled on the following.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// This will create a `users.settings` keyspace with a two keys.
CREATE KEYSPACE users.settings WITH KEYS username, setting
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By defining the keyspaces this way, we can not only setup multiple attributes for a particular keyspace, but we can also use the order to declare a hierarchy or priority for the keys. This hierarchy is what will also give us the ability to do efficient range scans. This subtle design may limit the query capability, but will help ensure applications are built to use the databases strengths.&lt;/p&gt;

&lt;p&gt;Also, by defining the name of the keyspace &lt;code&gt;users.settings&lt;/code&gt; it is immediately apparent as to what the values contain along with the attributes of the keys.&lt;/p&gt;

&lt;h4 id=&#34;select:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;SELECT&lt;/h4&gt;

&lt;p&gt;PrefixDB wouldn&amp;rsquo;t be much of a database without being able to read data. Here&amp;rsquo;s how it&amp;rsquo;s done.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT FROM users WHERE username = &amp;quot;eliquious&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above query would return the value stored in the &lt;code&gt;users&lt;/code&gt; keyspace under the key &lt;code&gt;eliquious&lt;/code&gt;. What about multiple keys?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT FROM users.settings WHERE user = &amp;quot;eliquious&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query above is cool and here&amp;rsquo;s why. If you look at how the keyspace is defined above, there are two keys (&lt;code&gt;username&lt;/code&gt; and &lt;code&gt;setting&lt;/code&gt;). If we allow users to exclude attributes then we get some very neat semantics. However, they can&amp;rsquo;t just exclude any attribute, they must query them in the order they were defined in the keyspace.&lt;/p&gt;

&lt;p&gt;This limitation might seem bizarre for those coming from the traditional RDBMS world. However, we&amp;rsquo;re focusing on efficient range queries and this limitation will give us a significant boost in query performance. This also changes how applications must define the keyspaces and in doing so reap the benefits of using PrefixDB.&lt;/p&gt;

&lt;p&gt;Even though this limitation exists, you can sill query for values using all the attributes in a key.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT FROM users.settings WHERE user = &amp;quot;eliquious&amp;quot; AND setting = &amp;quot;email&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query above would return the email for the user &lt;code&gt;eliquious&lt;/code&gt;. Additionally, the efficient range scans would allow for queries like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT FROM users.messages WHERE user = &amp;quot;eliquious&amp;quot; AND
    time BETWEEN &amp;quot;2016-12-16&amp;quot; AND &amp;quot;2016-01-16&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;upsert:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;UPSERT&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;UPSERT&lt;/code&gt; is not used in many databases, however, it is used to insert the value if it doesn&amp;rsquo;t exist or update it if the key is already there. Here&amp;rsquo;s how it looks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPSERT &amp;quot;...&amp;quot; INTO users WHERE username = &amp;quot;eliquious&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It works similarly if the keyspace has multiple attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPSERT &amp;quot;Bugs Bunny&amp;quot; INTO users.setting WHERE username = &amp;quot;bugs.bunny&amp;quot; AND setting = &amp;quot;name&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;delete:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;DELETE&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;DELETE&lt;/code&gt; has the same semantics as &lt;code&gt;SELECT&lt;/code&gt; in terms of querying. For instance, all the attributes for a keyspace are not required for deletion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DELETE FROM users WHERE username = &amp;quot;elmer.fudd&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;drop-keyspace:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;DROP KEYSPACE&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;DROP&lt;/code&gt; deletes an entire keyspace.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DROP KEYSPACE users;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;conclusion:b74c517924e3e9b7d099e040e9ec53c5&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Obviously, this is just the start. Next time we&amp;rsquo;ll get into code.&lt;/p&gt;

&lt;p&gt;Even with only 5 query statements we have quite a bit of functionality. Statements that could be added in the future are &lt;code&gt;DESCRIBE&lt;/code&gt; (for describing keyspaces) and &lt;code&gt;BATCH&lt;/code&gt; (for large inserts).&lt;/p&gt;

&lt;p&gt;There are also a few extra clauses that may be beneficial in the short term. Such as &lt;code&gt;LIMIT&lt;/code&gt; and possibly &lt;code&gt;ORDER BY&lt;/code&gt;, although &lt;code&gt;ORDER BY&lt;/code&gt; might be limited as well for efficiency.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>